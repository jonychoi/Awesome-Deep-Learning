# About Optimizers

**Jonathan Choi 2021**

**[Deep Learning By Torch] End to End study scripts of Deep Learning by implementing code practice with Pytorch.**

If you have an any issue, please PR below.

[[Deep Learning By Torch] - Github @JonyChoi](https://github.com/jonychoi/Deep-Learning-By-Torch)

Here, we are going to learn about the various optimizers, used in various practices in fields. I scraped about what are optimizers and how it works, including various architectures of them.

Reference From

1. Deep Learning Optimizers

https://medium.com/mlearning-ai/deep-learning-optimizers-4c13d0799b4d

2. Various Optimization Algorithms For Training Neural Network

https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6

3. How to pick the best learning rate for your machine learning project

https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2

---

# Deep Learning Optimizers

By Antony Christopher

![optimizers](https://miro.medium.com/max/1400/0*ohZCvcov5Lv8U05G.png)

In Deep Learning the optimizers play an important role. It takes the key role in losses. Basically, the optimizers do reduce the losses. How do the optimizers play in Deep Learning? Well, coming to this deep learning model if any changes in weight in the input layer the output layer gets changed. Further layer gets used of the weight. As things go on, we might be ended up with model accuracy is very low. In such a situation, we can use the optimizers to reduce the loss. Let start with the basics about the Gradient Descent that leads to different optimization algorithms.

## What is Gradient?

> “A gradient measures how much the output of a function changes if you change the inputs a little bit.” — Lex Fridman (MIT)

A gradient simply measures the change in weight with respect to the change in the error. The gradient can be termed as the slope of the function. If the gradient is higher then the slope will be steeper and faster the model can learn. If the slope is zero, the model stops learning. In mathematical terms, the gradient is the partial derivative of the inputs.

## Gradient Descent

Gradient Descent is an optimization algorithm for finding the local minimum of a differential function. It is used to find the values of a function’s parameters that minimize a cost function as far as possible.

You have defined the value of the initial parameter from the gradient descent calculates iteratively, so the value becomes minimize which gives the cost function.

Let’s explain in a better understanding. At this point image a person at the top of the mountain. The goal is to reach the base camp, say the bottom of the mountain. Here the problem is the person is blind. How come the person reaches the bottom?. Well, you have to take small steps and move towards the direction of the higher incline. Continue this step iteratively until reaches the base camp at the bottom. This is how works in Gradient Descent.

![](https://miro.medium.com/max/450/0*2Y1fw73f2XP6QpEr.jpg)

Imagine the image below illustrates our hill from a top-down view and the red arrows are the steps of our climber. Think of a gradient in this context as a vector that contains the direction of the steepest step the blindfolded man can take and also how long that step should be.

![](https://miro.medium.com/max/723/0*fvViPj_Rso_gZfR8.png)

Note that the gradient ranging from X0 to X1 is much longer than the one reaching from X3 to X4. This is because the steepness/slope of the hill, which determines the length of the vector, is less. This perfectly represents the example of the hill because the hill is getting less steep the higher it’s climbed. Therefore a reduced gradient goes along with a reduced slope and a reduced step size for the hill climber.

## How Gradient descent Works?

The equation below describes what gradient descent does: b is the next position of our climber, while a represents his current position. The minus sign refers to the minimization part of gradient descent. The gamma in the middle is a waiting factor and the gradient term ( Δf(a) ) is simply the direction of the steepest descent.

![](https://miro.medium.com/max/1050/0*JlL4BOf3t8ZQ09JF.png)

So this formula basically tells us the next position we need to go, which is the direction of the steepest descent. Let’s look at another example to really drive the concept home.

Imagine you have a machine learning problem and want to train your algorithm with gradient descent to minimize your cost-function J(w, b) and reach its local minimum by tweaking its parameters (w and b). The image below shows the horizontal axes represent the parameters (w and b), while the cost function J(w, b) is represented on the vertical axes. Gradient descent is a convex function.

![](https://miro.medium.com/max/1050/0*9PyjoLHeB-RJ3gjF.png)


We know we want to find the values of w and b that correspond to the minimum of the cost function (marked with the red arrow). To start finding the right values we initialize w and b with some random numbers. Gradient descent then starts at that point (somewhere around the top of our illustration), and it takes one step after another in the steepest downside direction (i.e., from the top to the bottom of the illustration) until it reaches the point where the cost function is as small as possible.

## What is the Learning Rate?

In Gradient Descent algorithm plays a vital role in the learning rate. It shows how many steps need to take compared to the previous one. Let’s take the climber example as above, on top taking each step is determined by the learning rate. In simple steps as (Learning Rate * old step). If the magnitude is 4.2 and the learning rate is 0.01, the next step will be 0.042. Which is away from the previous one. Hope this clarifies why the learning rate is important in GD.

![](https://miro.medium.com/max/1050/0*TijuAiTTaXC373MD.png)

If the learning rate chooses the as a big one, the climber never reaches the bottom or the model doesn’t reach a local minimum. If the learning rate is small then the model reaches a local minimum as expected.

## Gradient Descent variants

There are three variants of gradient descent based on the amount of data used to calculate the gradient:

- Batch gradient descent
- Stochastic gradient descent
- Mini-batch gradient descent

## Batch Gradient Descent

Batch Gradient Descent, aka Vanilla gradient descent, calculates the error for each observation in the dataset but performs an update only after all observations have been evaluated.

Batch gradient descent is not often used, because it represents a huge consumption of computational resources, as the entire dataset needs to remain in memory.

## Stochastic Gradient Descent

Stochastic gradient descent (SGD) performs a parameter update for each observation. So instead of looping over each observation, it just needs one to perform the parameter update. SGD is usually faster than batch gradient descent, but its frequent updates cause a higher variance in the error rate, which can sometimes jump around instead of decreasing.

![](https://miro.medium.com/max/726/0*R5Doj41JhYPOemIp.png)

## Mini-Batch Gradient Descent

It is a combination of both bath gradient descent and stochastic gradient descent. Mini-batch gradient descent performs an update for a batch of observations. It is the algorithm of choice for neural networks, and the batch sizes are usually from 50 to 256.

![](https://miro.medium.com/max/960/0*4jrMGp8t1pMG-rtH.png)

## SGD with Momentum

The SGD with momentum is always faster than the SGD. Basically, momentum plays a key role here. The momentum leads to accelerates the gradient in the right direction and leads to faster convergence. Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind it. Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom.

Recall that Gradient Descent simply updates the weights θ by directly subtracting the gradient of the cost function J(θ) with regards to the weights (∇θJ(θ)) multiplied by the learning rate η. The equation is θ ← θ — η∇θJ(θ). It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.

Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate η), and it updates the weights by simply adding this momentum vector (see below equation). In other words, the gradient is used for acceleration, not for speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, simply called the momentum, which must be set between 0 (high friction) and 1(no friction). A typical momentum value is 0.9.

Momentum algorithm

1. $m ← βm − η∇θJ(θ)$
2. $θ ←θ + m$

You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate η multiplied by 1 − β (ignoring the sign). For example, if β = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so Momentum optimization ends up going 10 times faster than Gradient Descent! This allows Momentum optimization to escape from plateaus much faster than Gradient Descent.

Gradient Descent goes down the steep slope quite fast, but then it takes a very long time to go down the valley. In contrast, Momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum).

Implementing Momentum optimization in Keras is a no-brainer: just use the SGD optimizer and set its momentum hyperparameter, then lie back and profit!

```optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)```

Nesterov Accelerated Gradient
The idea of Nesterov Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. The only difference between Momentum optimization is that the gradient is measured at θ + βm rather than at θ.

## Nesterov Accelerated Gradient algorithm

1. $m →βm − η∇θJ θ + βm$
2. $θ →θ + m$

This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position, as you can see in below figure where ∇1 represents the gradient of the cost function measured at the starting point θ, and ∇2 represents the gradient at the point located at θ + βm). As you can see, the Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular Momentum optimization. Moreover, note that when the momentum pushes the weights across a valley, ∇1 continues to push further across the valley, while ∇2 pushes back toward the bottom of the valley. This helps reduce oscillations and thus converges faster.

![](https://miro.medium.com/max/1050/1*dftQDckpFRFxcghEs935Bw.png)

NAG will almost always speed up training compared to regular Momentum optimization.

To use it, simply set nesterov=True when creating the SGD optimizer:

```optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)```

## AdaGrad

Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum.

The AdaGrad algorithm achieves this by scaling down the gradient vector along the steepest dimensions

AdaGrad algorithm

![](https://miro.medium.com/max/657/1*peVp4fMeivSXsBcgtEtN7w.png)

The first step accumulates the square of the gradients into the vector s (recall that the ⊗ symbol represents the element-wise multiplication). In other words, each si accumulates the squares of the partial derivative of the cost function with regards to parameter θi. If the cost function is steep along the ith dimension, then si will get larger and larger at each iteration.

The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor (the ⊘ symbol represents the element-wise division, and ϵ is a smoothing term to avoid division by zero, typically set to 10–10). This vectorized form is equivalent to computing θi for all parameters θi (simultaneously).

In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum. One additional benefit is that it requires much less tuning of the learning rate hyperparameter η.

![](https://miro.medium.com/max/1050/1*9670Po2urfTbSsTQIWhBuQ.png)

AdaGrad often performs well for simple quadratic problems, but unfortunately, it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). However, understanding Adagrad is helpful to grasp the other adaptive learning rate optimizers.

## RMSProp

Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.

RMSProp algorithm

![](https://miro.medium.com/max/789/1*UvxyJZlSWpvWSByM8-mZ0A.png)

The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.

As you might expect, Keras has an RMSProp optimizer:

```optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)```

Except for very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around.

## Adam

Adam, which stands for adaptive moment estimation, combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients

Adam Algorithm

![](https://miro.medium.com/max/828/1*ekfWSBYkBQ-F7XobQ4OlCg.png)

*t represents the iteration number (starting at 1).

If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both Momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just 1 — β1 times the decaying sum). Steps 3 and 4 are somewhat of technical detail: since m and s are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost m and s at the beginning of training.

The momentum decay hyperparameter β1 is typically initialized to 0.9, while the scaling decay hyperparameter β2 is often initialized to 0.999. As earlier, the smoothing term ϵ is usually initialized to a tiny number such as 10–7.

```optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)```


Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter η. You can often use the default value η = 0.001, making Adam even easier to use than Gradient Descent.

Hope this article gives you a better understanding of optimizers. See you soon on new one :)

---

# Various Optimization Algorithms For Training Neural Network
## The right optimization algorithm can reduce training time exponentially.
By Sanket Doshi


Many people may be using optimizers while training the neural network without knowing that the method is known as optimization. Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.

![](https://miro.medium.com/max/867/1*65Mxg_Yfq-L7AvaS0K5aGA.png)

Optimizers help to get results faster

How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.

We’ll learn about different types of optimizers and their advantages:

## Gradient Descent

Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm.

Gradient descent is a first-order optimization algorithm which is dependent on the first order derivative of a loss function. It calculates that which way the weights should be altered so that the function can reach a minima. Through backpropagation, the loss is transferred from one layer to another and the model’s parameters also known as weights are modified depending on the losses so that the loss can be minimized.

algorithm: **θ=θ−α⋅∇J(θ)**

**Advantages:**

1. Easy computation.
2. Easy to implement.
3. Easy to understand.

**Disadvantages:**

1. May trap at local minima.
2. Weights are changed after calculating gradient on the whole dataset. So, if the dataset is too large than this may take years to converge to the minima.
3. Requires large memory to calculate gradient on the whole dataset.

## Stochastic Gradient Descent

It’s a variant of Gradient Descent. It tries to update the model’s parameters more frequently. In this, the model parameters are altered after computation of loss on each training example. So, if the dataset contains 1000 rows SGD will update the model parameters 1000 times in one cycle of dataset instead of one time as in Gradient Descent.

**θ=θ−α⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.**

As the model parameters are frequently updated parameters have high variance and fluctuations in loss functions at different intensities.

**Advantages:**

1. Frequent updates of model parameters hence, converges in less time.
2. Requires less memory as no need to store values of loss functions.
3. May get new minima’s.

**Disadvantages:**

1. High variance in model parameters.
2. May shoot even after achieving global minima.
3. To get the same convergence as gradient descent needs to slowly reduce the value of learning rate.

## Mini-Batch Gradient Descent
It’s best among all the variations of gradient descent algorithms. It is an improvement on both SGD and standard gradient descent. It updates the model parameters after every batch. So, the dataset is divided into various batches and after every batch, the parameters are updated.

**θ=θ−α⋅∇J(θ; B(i)), where {B(i)} are the batches of training examples.**

**Advantages:**

1. Frequently updates the model parameters and also has less variance.
2. Requires medium amount of memory.

**All types of Gradient Descent have some challenges:**

1. Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge.
2. Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate.
3. May get trapped at local minima.

## Momentum
Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by **‘γ’**.

**V(t)=γV(t−1)+α.∇J(θ)**

Now, the weights are updated by **θ=θ−V(t)**.

The momentum term **γ** is usually set to 0.9 or a similar value.

**Advantages:**

1. Reduces the oscillations and high variance of the parameters.
2. Converges faster than gradient descent.

**Disadvantages:**

1. One more hyper-parameter is added which needs to be selected manually and accurately.

## Nesterov Accelerated Gradient

Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed. It is a look ahead method. We know we’ll be using **γV(t−1)** for modifying the weights so, **θ−γV(t−1)** approximately tells us the future location. Now, we’ll calculate the cost based on this future parameter rather than the current one.

**V(t)=γV(t−1)+α. ∇J( θ−γV(t−1) )** and then update the parameters using **θ=θ−V(t)**.

![]()https://miro.medium.com/max/389/1*aPPq25NxIQB_lP1fhqRNkg.png
NAG vs momentum at local minima

**Advantages:**

1. Does not miss the local minima.
2. Slows if minima’s are occurring.

**Disadvantages:**

1. Still, the hyperparameter needs to be selected manually.

## Adagrad
One of the disadvantages of all the optimizers explained is that the learning rate is constant for all parameters and for each cycle. This optimizer changes the learning rate. It changes the learning rate **‘η’** for each parameter and at every time step ‘t’. It’s a type second order optimization algorithm. It works on the derivative of an error function.

![](https://miro.medium.com/max/320/1*zCRt57Wf8KYkvYmqWFyqew.png)
A derivative of loss function for given parameters at a given time t.

![](https://miro.medium.com/max/470/1*QKKVHVeX312PJN7pCPtgGw.png)
Update parameters for given input i and at time/iteration t

**η** is a learning rate which is modified for given parameter **θ(i)** at a given time based on previous gradients calculated for given parameter **θ(i)**.

We store the sum of the squares of the gradients w.r.t. θ(i) up to time step t, while ϵ is a smoothing term that avoids division by zero (usually on the order of 1e−8). Interestingly, without the square root operation, the algorithm performs much worse.

It makes big updates for less frequent parameters and a small step for frequent parameters.

**Advantages:**

1. Learning rate changes for each training parameter.
2. Don’t need to manually tune the learning rate.
3. Able to train on sparse data.

**Disadvantages:**

1. Computationally expensive as a need to calculate the second order derivative.
2. The learning rate is always decreasing results in slow training.

## AdaDelta

It is an extension of **AdaGrad** which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, ***Adadelta*** limits the window of accumulated past gradients to some fixed size **w**. In this exponentially moving average is used rather than the sum of all the gradients.

**E[g²](t)=γ.E[g²](t−1)+(1−γ).g²(t)**

We set γ to a similar value as the momentum term, around 0.9.

![](https://miro.medium.com/max/609/1*TVdJhPPIaSNsQe95yKApmA.png)

Update the parameters

**Advantages:**

1. Now the learning rate does not decay and the training does not stop.

**Disadvantages:**

1. Computationally expensive.

## Adam

[Adam](https://arxiv.org/pdf/1412.6980.pdf) (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like **AdaDelta**, ***Adam*** also keeps an exponentially decaying average of past gradients **M(t)**.

**M(t)** and **V(t)** are values of the first moment which is the **Mean** and the second moment which is the **uncentered variance** of the gradients respectively.

![](https://miro.medium.com/max/234/1*uhvftel2AgBioJkgaYdRiA.png)
First and second order of momentum

Here, we are taking mean of **M(t)** and **V(t)** so that **E[m(t)]** can be equal to **E[g(t)]** where, **E[f(x)]** is an expected value of **f(x)**.

To update the parameter:

![](https://miro.medium.com/max/390/1*kl2TXe-A5C7UEQEgCIC_hQ.png)

Update the parameters

The values for β1 is 0.9 , 0.999 for β2, and (10 x exp(-8)) for ‘ϵ’.

**Advantages:**

1. The method is too fast and converges rapidly.
2. Rectifies vanishing learning rate, high variance.

**Disadvantages:**

Computationally costly.

## Comparison between various optimizers

![](https://miro.medium.com/max/900/1*_osB82GKHBOT8k1idLqiqA.gif)

Comparison 1

![](https://miro.medium.com/max/900/1*_osB82GKHBOT8k1idLqiqA.gif)

comparison 2

## Conclusions

Adam is the best optimizers. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.

For sparse data use the optimizers with dynamic learning rate.

If, want to use gradient descent algorithm than min-batch gradient descent is the best option.

I hope you guys liked the article and were able to give you a good intuition towards the different behaviors of different Optimization Algorithms.

---

# How to pick the best learning rate for your machine learning project
By David Mack

A common problem we all face when working on deep learning projects is choosing a learning rate and optimizer (the hyper-parameters). If you’re like me, you find yourself guessing an optimizer and learning rate, then checking if they work (and we’re not alone).

### To better understand the affect of optimizer and learning rate choice, I trained the same model 500 times. The results show that the right hyper-parameters are crucial to training success, yet can be hard to find.

Finally, I’ll discuss solutions to this problem, using automated methods to choose optimal hyper-parameters.

## Experimental setup

I trained the basic convolutional neural network from TensorFlow’s tutorial series, which learns to recognize MNIST digits. This is a reasonably small network, with two convolutional layers and two dense layers, a total of roughly 3,400 weights to train. The same random seed is used for each training.

It should be noted that the results below are for one specific model and dataset. The ideal hyper-parameters for other models and datasets will differ.

## Which learning rate works best?

The first thing we’ll explore is how learning rate affects model training. In each run the same model is trained from scratch, varying only the optimizer and learning rate.

The model was trained with 6 different optimizers: Gradient Descent, Adam, Adagrad, Adadelta, RMS Prop and Momentum. For each optimizer it was trained with 48 different learning rates, from 0.000001 to 100 at logarithmic intervals.

In each run, the network is trained until it achieves at least 97% train accuracy. The maximum time allowed was 120 seconds. The experiments were run on an Nvidia Tesla K80, hosted by FloydHub. The source code is [available for download](https://github.com/Octavian-ai/learning-rates).

Here is the training time for each choice of learning rate and optimizer:

![](https://miro.medium.com/max/2000/1*gUHTqcK1PYR1EfyYAiCrmQ.png)

### Note that a time of 120 seconds means the network failed to train.

The above graph is interesting. We can see that:

- For every optimizer, the majority of learning rates fail to train the model.
- There is a valley shape for each optimizer: too low a learning rate never progresses, too high a learning rate causes instability and never converges. In between there is a band of “just right” learning rates that successfully train.
- There is no learning rate that works for all optimizers.
- Learning rate can affect training time by an order of magnitude.

Summarizing the above, it’s crucial you choose the correct learning rate as otherwise your network will either fail to train, or take much longer to converge.

To illustrate how each optimizer differs in its optimal learning rate, here is the the fastest and slowest model to train for each learning rate, across all optimizers. Notice that the maximum time is 120s (e.g. network failed to train) across the whole graph — there is no single learning rate that works for every optimizers:

![](https://miro.medium.com/max/2000/1*cPcEEJJe1EikWQVeUB8VdQ.png)

Another observation on the above graph is the wide range of learning rates (from 0.001 to 30) that achieve success with at least one optimizer.

## Which optimizer performs best?

Now that we’ve identified the best learning rates for each optimizer, let’s compare the performance of each optimizer training with the best learning rate found for it in the previous section.

Here is the validation accuracy of each optimizer over time. This lets us observe how quickly, accurately and stably each performs:

![](https://miro.medium.com/max/2000/1*3mbLR7aSgbg_UoueBymw5g.png)

(Note that this training was run much slower than the earlier experiments, with frequent pauses to evaluate, so I could capture higher resolution)

A few observations:

- All of the optimizers, apart from RMSProp (see final point), manage to converge in a reasonable time.
- Adam learns the fastest.
- Adam is more stable than the other optimizers, it doesn’t suffer any major decreases in accuracy.
- RMSProp was run with the default arguments from TensorFlow (decay rate 0.9, epsilon 1e-10, momentum 0.0) and it could be the case these do not work well for this task. This is a good use case for automated hyper-parameter search (see the last section for more about that).

Adam also had a relatively wide range of successful learning rates in the previous experiment. Overall, Adam is the best choice of our six optimizers for this model and dataset.

## How does model size affect training time?

Now lets look at how the size of the model affects how it trains.

We’ll vary the model size by a linear factor. That factor will linearly scale the number of convolutional filters and the width of the first dense layer, thus approximately linearly scaling the total number of weights in the model.

There are two aspects we’ll investigate:

1. How does the training time change as the model grows, for a fixed optimizer and training rate?
2. Which learning rate trains fastest on each size of model, for a fixed optimizer?

## How does training time change as the model grows?
Below shows the time taken to achieve 96% training accuracy on the model, increasing its size from 1x to 10x. We’ve used one of our most successful hyper-parameters from earlier:

![](https://miro.medium.com/max/1050/1*XkWAKrH-HRqsrZEcVmXqVg.png)

Red line is the data, grey dotted line is a linear trend-line, for comparison

- The time to train grows linearly with the model size.
- The same learning rate successfully trains the network across all model sizes.

(Note: the following results can only be relied upon for the dataset and models tested here, but could be worth testing for your experiments.)

This is a nice result. Our choice of hyper-parameters was not invalidated by linearly scaling the model. This may hint that hyper-parameter search can be performed on a scaled-down version of a network, to save on computation time.

This also shows that as the network gets bigger it doesn’t incur any O(n²) work in converging the model (the linear growth in time can be explained by the extra operations incurred for each weight’s training).

This result is further reassuring, as it shows our deep learning framework (here TensorFlow) is scales efficiently.

## Which learning rate performs best for different sizes of model?

Let’s run the same experiment for multiple learning rates and see how training time responds to model size:

![](https://miro.medium.com/max/2000/1*lSThb8PKYoNvbCxXVAIwYA.png)

Failed runs are shown as missing points and disconnected lines

- Learning rates 0.0005, 0.001, 0.00146 performed best — these also performed best in the first experiment. We see here the same “sweet spot” band as in the first experiment.
- Each learning rate’s time to train grows linearly with model size.
- Learning rate performance did not depend on model size, the same rates that performed best for 1x size performed best for 10x size.
- Above 0.001, increasing the learning rate increased the time to train and also increased the variance in training time (as compared to a linear function of model size).
- Time to train can roughly be modeled as c + kn for a model with n weights, fixed cost c and learning constant k=f(learning rate).
In summary, the best performing learning rate for size 1x was also the best learning rate for size 10x.

## Automating choice of learning rate

As the earlier results show, it’s crucial for model training to have an good choice of optimizer and learning rate.
Manually choosing these hyper-parameters is time-consuming and error-prone. As your model changes, the previous choice of hyper-parameters may no longer be ideal. It is impractical to continually perform new searches by hand.
There are a number of ways to automatically pick hyper-parameters. I’ll outline a couple of different approaches here.

## Grid search
[Grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) is what we performed in the first experiment —for each hyper-parameter, create a list of possible values. Then for each combination of possible hyper-parameter values, train the network and measure how it performs. The best hyper-parameters are those that give the best observed performance.
Grid search is very easy to implement and understand. It’s also easy to verify that you’ve searched a sufficiently broad section of the parameter search. It’s very popular in research because of these reasons.

## Population based training
[Population based training (DeepMind)](https://deepmind.com/blog/article/population-based-training-neural-networks) is an elegant implementation of using a [genetic algorithm](https://en.wikipedia.org/wiki/Evolutionary_algorithm) for hyper-parameter choice.

In PBT, a population of models are created. They are all continuously trained in parallel. When any member of the population has had sufficiently long to train to show improvement, its validation accuracy is compared to the rest of the population. If its performance is in the lowest 20%, then it copies and mutates the hyper-parameters and variables of one of the top 20% performers.

In this way, the most successful hyper-parameters spawn many slightly mutated variants of themselves and the best hyper-parameters are likely discovered.

## Next steps

Thanks for reading this investigation into learning rates. I began these experiments out of my own curiosity and frustration around hyper-parameter turning, and I hope you enjoy the results and conclusions as much as I have.

If there is a particular topic or extension you’re interested in seeing, let me know. Also, if you’re interested in donating some GPU time to run a much bigger version of this experiment, I’d love to talk.

These writings are part of a year-long exploration of AI architecture topics. Follow this publication (and give this article some applause!) to get updates when the next pieces come out.

---


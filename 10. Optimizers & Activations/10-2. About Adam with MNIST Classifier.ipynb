{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9.1: About Adam with MNIST Classifier\n",
    "\n",
    "**Jonathan Choi 2021**\n",
    "\n",
    "**[Deep Learning By Torch] End to End study scripts of Deep Learning by implementing code practice with Pytorch.**\n",
    "\n",
    "If you have an any issue, please PR below.\n",
    "\n",
    "[[Deep Learning By Torch] - Github @JonyChoi](https://github.com/jonychoi/Deep-Learning-By-Torch)\n",
    "\n",
    "In this lab(9.0 and 9.1), we are going to learn about various optimizers, including SGD(Stochastic Gradient Descent) as we used always, besides about Adam, Adagrad, Momentum, GD, Adadelta, RMSProp etc. We are going to create the neural network using optimizer Adam at the End. Please read script \"09.0 About optimizers\" to get more understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimizers](https://cdn-images-1.medium.com/max/2000/1*3mbLR7aSgbg_UoueBymw5g.png)\n",
    "\n",
    "Reference from \n",
    "\n",
    "https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root='MNIST_data/',\n",
    "                             train=True,\n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "mnist_test = datasets.MNIST(root='MNIST_data/',\n",
    "                            train=False,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, shuffle=True, drop_last=True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with SGD and ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer):\n",
    "    #set optimizer\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "        \n",
    "        for X, Y in data_loader:\n",
    "\n",
    "            #reshape input image into (batchsize x 784)\n",
    "            #label is not one-hot encoded\n",
    "            X = X.view(-1, 28 * 28).to(device)# Before X.shape = torch.Size([100, 1, 28, 28]) After torch.Size([100, 784])\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            #prediction\n",
    "            pred = model(X)\n",
    "\n",
    "            #cost\n",
    "            cost = F.cross_entropy(pred, Y)\n",
    "\n",
    "            #Reduce the cost\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += cost / total_batch\n",
    "\n",
    "        print('Epoch: {:d}/15, Cost: {:.6f}'.format(epoch+1, avg_cost))\n",
    "\n",
    "    print('Learning Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15, Cost: 12.765790\n",
      "Epoch: 2/15, Cost: 10.279278\n",
      "Epoch: 3/15, Cost: 8.915923\n",
      "Epoch: 4/15, Cost: 7.992557\n",
      "Epoch: 5/15, Cost: 7.297637\n",
      "Epoch: 6/15, Cost: 6.743856\n",
      "Epoch: 7/15, Cost: 6.280459\n",
      "Epoch: 8/15, Cost: 5.879832\n",
      "Epoch: 9/15, Cost: 5.527321\n",
      "Epoch: 10/15, Cost: 5.214455\n",
      "Epoch: 11/15, Cost: 4.935022\n",
      "Epoch: 12/15, Cost: 4.684261\n",
      "Epoch: 13/15, Cost: 4.458256\n",
      "Epoch: 14/15, Cost: 4.253722\n",
      "Epoch: 15/15, Cost: 4.067929\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "model = LinearMNISTClassifier().to(device)\n",
    "train(data_loader, model, 'sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4203999936580658\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "#Test the model using test sets\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1 ,28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    pred = model(X_test)\n",
    "    correct_prediction = torch.argmax(pred, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "\n",
    "    print('Accuracy: ', accuracy.item())\n",
    "\n",
    "    #Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "\n",
    "    #below X_single_data.shape => torch.size([1, 784])\n",
    "\n",
    "    #X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    #Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    #X_test[r].shape => torch.size([784])\n",
    "    #X_test[r:r+1].shape => torch.size([1, 784])\n",
    "    #if torch.argmax(single_prediction, 1) => Since just torch.size([784]) makes IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "    #else torch.argmax(single_prediction, 0) => no error with size([784])\n",
    "    X_single_data = X_test[r]\n",
    "    Y_single_data = Y_test[r]\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 0).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15, Cost: 5.672674\n",
      "Epoch: 2/15, Cost: 1.664780\n",
      "Epoch: 3/15, Cost: 1.087722\n",
      "Epoch: 4/15, Cost: 0.856496\n",
      "Epoch: 5/15, Cost: 0.727496\n",
      "Epoch: 6/15, Cost: 0.643582\n",
      "Epoch: 7/15, Cost: 0.584310\n",
      "Epoch: 8/15, Cost: 0.541188\n",
      "Epoch: 9/15, Cost: 0.508095\n",
      "Epoch: 10/15, Cost: 0.481329\n",
      "Epoch: 11/15, Cost: 0.459101\n",
      "Epoch: 12/15, Cost: 0.440584\n",
      "Epoch: 13/15, Cost: 0.424890\n",
      "Epoch: 14/15, Cost: 0.411225\n",
      "Epoch: 15/15, Cost: 0.399128\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "model = LinearMNISTClassifier().to(device)\n",
    "train(data_loader, model, 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8872999548912048\n",
      "torch.Size([1, 784]) torch.Size([1, 784])\n",
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    }
   ],
   "source": [
    "#Test the model using test sets\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.data.view(-1 ,28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.targets.to(device)\n",
    "\n",
    "    pred = model(X_test)\n",
    "    correct_prediction = torch.argmax(pred, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "\n",
    "    print('Accuracy: ', accuracy.item())\n",
    "\n",
    "    #Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "\n",
    "    print(mnist_test.data[r: r+1].view(-1, 28 * 28).shape, X_test[r: r+1].shape)\n",
    "\n",
    "    #below X_single_data.shape => torch.size([1, 784])\n",
    "\n",
    "    #X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    #Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    #X_test[r].shape => torch.size([784])\n",
    "    #X_test[r:r+1].shape => torch.size([1, 784])\n",
    "    #if torch.argmax(single_prediction, 1) => Since just torch.size([784]) makes IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "    #else torch.argmax(single_prediction, 0) => no error with size([784])\n",
    "    X_single_data = X_test[r]\n",
    "    Y_single_data = Y_test[r]\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

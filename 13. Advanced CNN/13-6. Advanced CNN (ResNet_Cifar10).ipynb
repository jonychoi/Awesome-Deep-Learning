{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10.6.2: Advanced CNN (Resnet with CIFAR10)\n",
    "\n",
    "**Jonathan Choi 2021**\n",
    "\n",
    "**[Deep Learning By Torch] End to End study scripts of Deep Learning by implementing code practice with Pytorch.**\n",
    "\n",
    "If you have an any issue, please PR below.\n",
    "\n",
    "[[Deep Learning By Torch] - Github @JonyChoi](https://github.com/jonychoi/Deep-Learning-By-Torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = visdom.Visdom()\n",
    "vis.close(env = \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Value Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_tracker(value_plot, value, index):\n",
    "    '''num, loss_value are Tensor'''\n",
    "    vis.line(X = index, Y = value, win = value_plot, update = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "### How to Calculate mean and std in Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n",
      "[125.30691805 122.95039414 113.86538318]\n",
      "[62.99321928 62.08870764 66.70489964]\n",
      "[0.49139968 0.48215841 0.44653091]\n",
      "[0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root = 'cifar10/', download = True, transform = transform, train = True)\n",
    "\n",
    "print(trainset.data.shape)\n",
    "\n",
    "train_data_mean = trainset.data.mean(axis = (0, 1, 2))\n",
    "train_data_std = trainset.data.std(axis = (0, 1, 2))\n",
    "\n",
    "print(train_data_mean)\n",
    "print(train_data_std)\n",
    "\n",
    "train_data_mean = train_data_mean / 255\n",
    "train_data_std = train_data_std / 255\n",
    "\n",
    "print(train_data_mean)\n",
    "print(train_data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_data_mean, train_data_std)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_data_mean, train_data_std)\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(download = True, root = 'cifar10/', transform = transform_train, train = True)\n",
    "testset = datasets.CIFAR10(download = True, root = 'cifar10/', transform = transform_test, train = False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = 256, shuffle = True, num_workers = 0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = testset, batch_size = 256, shuffle = True, num_workers = 0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horese', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reform the ResNet since the dataset resolution is too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.resnet as resnet\n",
    "from torch import Tensor\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "BasicBlock = resnet.BasicBlock\n",
    "Bottleneck = resnet.Bottleneck\n",
    "conv1x1 = resnet.conv1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #change strides from 2 to 1 below -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=1,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 10, True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Moment!!!!!!!!!!!!!!!!!\n",
    "\n",
    "if not normalize the Tensor just like ```torch.Tensor(1, 3, 32, 32).to(device)```\n",
    "\n",
    "it returns nan to all of classes\n",
    "\n",
    "**It is important to normalize our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buddhalight\\envs\\buddhalight\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2347, -0.2441,  0.2715,  0.1799,  0.0963, -0.1991,  0.0620, -0.3722,\n",
      "          0.0864,  0.4435]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(1, 3, 32, 32).to(device)\n",
    "out = resnet50(test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet50.parameters(), lr = 0.1, momentum =0.9, weight_decay = 5e-4)\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size =10, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plt = vis.line(Y=torch.Tensor(1).zero_(), opts = dict(title = \"loss_tracker\", legend = ['loss'], showlegend = True))\n",
    "acc_plt = vis.line(Y = torch.Tensor(1).zero_(), opts = dict(title = 'Accuracy', legend = ['Acc'], showlegend = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Acc_check Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_check(net, test_loader, epoch, save = 1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            #prediction\n",
    "            pred = net(X)\n",
    "\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "\n",
    "            total += Y.size(0)\n",
    "            correct += (predicted == Y).sum().item()\n",
    "\n",
    "    acc = (100 * correct / total)\n",
    "    print('Accuracy of the network on the 10000 test images: {}'.format(acc))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        if save:\n",
    "            torch.save(net.state_dict(), \"./models/resnet_cifar10/model_epoch_{}_acc_{}.pth\".format(epoch, acc))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with (acc check + model save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "Epoch: 1 / 20, MiniBatch: 29 / 196, Cost: 1.987879753112793\n",
      "Epoch: 1 / 20, MiniBatch: 59 / 196, Cost: 1.6570311784744263\n",
      "Epoch: 1 / 20, MiniBatch: 89 / 196, Cost: 1.669613003730774\n",
      "Epoch: 1 / 20, MiniBatch: 119 / 196, Cost: 1.70968759059906\n",
      "Epoch: 1 / 20, MiniBatch: 149 / 196, Cost: 1.6382006406784058\n",
      "Epoch: 1 / 20, MiniBatch: 179 / 196, Cost: 1.7769445180892944\n",
      "Accuracy of the network on the 10000 test images: 38.7\n",
      "Epoch: 2 / 20, MiniBatch: 29 / 196, Cost: 1.604959487915039\n",
      "Epoch: 2 / 20, MiniBatch: 59 / 196, Cost: 1.6588995456695557\n",
      "Epoch: 2 / 20, MiniBatch: 89 / 196, Cost: 1.7000317573547363\n",
      "Epoch: 2 / 20, MiniBatch: 119 / 196, Cost: 1.690345048904419\n",
      "Epoch: 2 / 20, MiniBatch: 149 / 196, Cost: 1.5970897674560547\n",
      "Epoch: 2 / 20, MiniBatch: 179 / 196, Cost: 1.5948519706726074\n",
      "Accuracy of the network on the 10000 test images: 38.92\n",
      "Epoch: 3 / 20, MiniBatch: 29 / 196, Cost: 1.669651985168457\n",
      "Epoch: 3 / 20, MiniBatch: 59 / 196, Cost: 1.578109860420227\n",
      "Epoch: 3 / 20, MiniBatch: 89 / 196, Cost: 1.6107028722763062\n",
      "Epoch: 3 / 20, MiniBatch: 119 / 196, Cost: 1.7158043384552002\n",
      "Epoch: 3 / 20, MiniBatch: 149 / 196, Cost: 1.6672979593276978\n",
      "Epoch: 3 / 20, MiniBatch: 179 / 196, Cost: 1.6427626609802246\n",
      "Accuracy of the network on the 10000 test images: 39.14\n",
      "Epoch: 4 / 20, MiniBatch: 29 / 196, Cost: 1.6343843936920166\n",
      "Epoch: 4 / 20, MiniBatch: 59 / 196, Cost: 1.5835413932800293\n",
      "Epoch: 4 / 20, MiniBatch: 89 / 196, Cost: 1.6438624858856201\n",
      "Epoch: 4 / 20, MiniBatch: 119 / 196, Cost: 1.683458685874939\n",
      "Epoch: 4 / 20, MiniBatch: 149 / 196, Cost: 1.724977731704712\n",
      "Epoch: 4 / 20, MiniBatch: 179 / 196, Cost: 1.6418060064315796\n",
      "Accuracy of the network on the 10000 test images: 39.05\n",
      "Epoch: 5 / 20, MiniBatch: 29 / 196, Cost: 1.642008662223816\n",
      "Epoch: 5 / 20, MiniBatch: 59 / 196, Cost: 1.7132502794265747\n",
      "Epoch: 5 / 20, MiniBatch: 89 / 196, Cost: 1.5460219383239746\n",
      "Epoch: 5 / 20, MiniBatch: 119 / 196, Cost: 1.682508111000061\n",
      "Epoch: 5 / 20, MiniBatch: 149 / 196, Cost: 1.6854939460754395\n",
      "Epoch: 5 / 20, MiniBatch: 179 / 196, Cost: 1.7777493000030518\n",
      "Accuracy of the network on the 10000 test images: 39.03\n",
      "Epoch: 6 / 20, MiniBatch: 29 / 196, Cost: 1.6886317729949951\n",
      "Epoch: 6 / 20, MiniBatch: 59 / 196, Cost: 1.680353045463562\n",
      "Epoch: 6 / 20, MiniBatch: 89 / 196, Cost: 1.585144281387329\n",
      "Epoch: 6 / 20, MiniBatch: 119 / 196, Cost: 1.7454557418823242\n",
      "Epoch: 6 / 20, MiniBatch: 149 / 196, Cost: 1.7004505395889282\n",
      "Epoch: 6 / 20, MiniBatch: 179 / 196, Cost: 1.6414577960968018\n",
      "Accuracy of the network on the 10000 test images: 39.06\n",
      "Epoch: 7 / 20, MiniBatch: 29 / 196, Cost: 1.580033540725708\n",
      "Epoch: 7 / 20, MiniBatch: 59 / 196, Cost: 1.627366542816162\n",
      "Epoch: 7 / 20, MiniBatch: 89 / 196, Cost: 1.6823846101760864\n",
      "Epoch: 7 / 20, MiniBatch: 119 / 196, Cost: 1.6452397108078003\n",
      "Epoch: 7 / 20, MiniBatch: 149 / 196, Cost: 1.7099201679229736\n",
      "Epoch: 7 / 20, MiniBatch: 179 / 196, Cost: 1.769403100013733\n",
      "Accuracy of the network on the 10000 test images: 39.13\n",
      "Epoch: 8 / 20, MiniBatch: 29 / 196, Cost: 1.7152318954467773\n",
      "Epoch: 8 / 20, MiniBatch: 59 / 196, Cost: 1.6473793983459473\n",
      "Epoch: 8 / 20, MiniBatch: 89 / 196, Cost: 1.6238384246826172\n",
      "Epoch: 8 / 20, MiniBatch: 119 / 196, Cost: 1.6833553314208984\n",
      "Epoch: 8 / 20, MiniBatch: 149 / 196, Cost: 1.751983642578125\n",
      "Epoch: 8 / 20, MiniBatch: 179 / 196, Cost: 1.64181649684906\n",
      "Accuracy of the network on the 10000 test images: 39.01\n",
      "Epoch: 9 / 20, MiniBatch: 29 / 196, Cost: 1.7322347164154053\n",
      "Epoch: 9 / 20, MiniBatch: 59 / 196, Cost: 1.6911438703536987\n",
      "Epoch: 9 / 20, MiniBatch: 89 / 196, Cost: 1.668939232826233\n",
      "Epoch: 9 / 20, MiniBatch: 119 / 196, Cost: 1.7342000007629395\n",
      "Epoch: 9 / 20, MiniBatch: 149 / 196, Cost: 1.6421308517456055\n",
      "Epoch: 9 / 20, MiniBatch: 179 / 196, Cost: 1.7873035669326782\n",
      "Accuracy of the network on the 10000 test images: 38.88\n",
      "Epoch: 10 / 20, MiniBatch: 29 / 196, Cost: 1.6934001445770264\n",
      "Epoch: 10 / 20, MiniBatch: 59 / 196, Cost: 1.668694257736206\n",
      "Epoch: 10 / 20, MiniBatch: 89 / 196, Cost: 1.6607221364974976\n",
      "Epoch: 10 / 20, MiniBatch: 119 / 196, Cost: 1.6296635866165161\n",
      "Epoch: 10 / 20, MiniBatch: 149 / 196, Cost: 1.706101417541504\n",
      "Epoch: 10 / 20, MiniBatch: 179 / 196, Cost: 1.6491038799285889\n",
      "Accuracy of the network on the 10000 test images: 39.16\n",
      "Epoch: 11 / 20, MiniBatch: 29 / 196, Cost: 1.6102516651153564\n",
      "Epoch: 11 / 20, MiniBatch: 59 / 196, Cost: 1.730950951576233\n",
      "Epoch: 11 / 20, MiniBatch: 89 / 196, Cost: 1.703076958656311\n",
      "Epoch: 11 / 20, MiniBatch: 119 / 196, Cost: 1.657356858253479\n",
      "Epoch: 11 / 20, MiniBatch: 149 / 196, Cost: 1.6531755924224854\n",
      "Epoch: 11 / 20, MiniBatch: 179 / 196, Cost: 1.5814833641052246\n",
      "Accuracy of the network on the 10000 test images: 38.96\n",
      "Epoch: 12 / 20, MiniBatch: 29 / 196, Cost: 1.731263518333435\n",
      "Epoch: 12 / 20, MiniBatch: 59 / 196, Cost: 1.6477997303009033\n",
      "Epoch: 12 / 20, MiniBatch: 89 / 196, Cost: 1.6824933290481567\n",
      "Epoch: 12 / 20, MiniBatch: 119 / 196, Cost: 1.6382399797439575\n",
      "Epoch: 12 / 20, MiniBatch: 149 / 196, Cost: 1.6251916885375977\n",
      "Epoch: 12 / 20, MiniBatch: 179 / 196, Cost: 1.6147524118423462\n",
      "Accuracy of the network on the 10000 test images: 39.16\n",
      "Epoch: 13 / 20, MiniBatch: 29 / 196, Cost: 1.6602740287780762\n",
      "Epoch: 13 / 20, MiniBatch: 59 / 196, Cost: 1.6351022720336914\n",
      "Epoch: 13 / 20, MiniBatch: 89 / 196, Cost: 1.6874055862426758\n",
      "Epoch: 13 / 20, MiniBatch: 119 / 196, Cost: 1.6995447874069214\n",
      "Epoch: 13 / 20, MiniBatch: 149 / 196, Cost: 1.647242546081543\n",
      "Epoch: 13 / 20, MiniBatch: 179 / 196, Cost: 1.6818114519119263\n",
      "Accuracy of the network on the 10000 test images: 38.93\n",
      "Epoch: 14 / 20, MiniBatch: 29 / 196, Cost: 1.717427134513855\n",
      "Epoch: 14 / 20, MiniBatch: 59 / 196, Cost: 1.682494044303894\n",
      "Epoch: 14 / 20, MiniBatch: 89 / 196, Cost: 1.6997138261795044\n",
      "Epoch: 14 / 20, MiniBatch: 119 / 196, Cost: 1.6115663051605225\n",
      "Epoch: 14 / 20, MiniBatch: 149 / 196, Cost: 1.5662286281585693\n",
      "Epoch: 14 / 20, MiniBatch: 179 / 196, Cost: 1.698138952255249\n",
      "Accuracy of the network on the 10000 test images: 38.86\n",
      "Epoch: 15 / 20, MiniBatch: 29 / 196, Cost: 1.6608275175094604\n",
      "Epoch: 15 / 20, MiniBatch: 59 / 196, Cost: 1.6589019298553467\n",
      "Epoch: 15 / 20, MiniBatch: 89 / 196, Cost: 1.7406139373779297\n",
      "Epoch: 15 / 20, MiniBatch: 119 / 196, Cost: 1.6459318399429321\n",
      "Epoch: 15 / 20, MiniBatch: 149 / 196, Cost: 1.7383484840393066\n",
      "Epoch: 15 / 20, MiniBatch: 179 / 196, Cost: 1.6356194019317627\n",
      "Accuracy of the network on the 10000 test images: 38.84\n",
      "Epoch: 16 / 20, MiniBatch: 29 / 196, Cost: 1.741283655166626\n",
      "Epoch: 16 / 20, MiniBatch: 59 / 196, Cost: 1.7021262645721436\n",
      "Epoch: 16 / 20, MiniBatch: 89 / 196, Cost: 1.7051156759262085\n",
      "Epoch: 16 / 20, MiniBatch: 119 / 196, Cost: 1.5408570766448975\n",
      "Epoch: 16 / 20, MiniBatch: 149 / 196, Cost: 1.6500537395477295\n",
      "Epoch: 16 / 20, MiniBatch: 179 / 196, Cost: 1.6973360776901245\n",
      "Accuracy of the network on the 10000 test images: 38.97\n",
      "Epoch: 17 / 20, MiniBatch: 29 / 196, Cost: 1.6869785785675049\n",
      "Epoch: 17 / 20, MiniBatch: 59 / 196, Cost: 1.659433126449585\n",
      "Epoch: 17 / 20, MiniBatch: 89 / 196, Cost: 1.6073474884033203\n",
      "Epoch: 17 / 20, MiniBatch: 119 / 196, Cost: 1.6420623064041138\n",
      "Epoch: 17 / 20, MiniBatch: 149 / 196, Cost: 1.6464067697525024\n",
      "Epoch: 17 / 20, MiniBatch: 179 / 196, Cost: 1.6674926280975342\n",
      "Accuracy of the network on the 10000 test images: 38.97\n",
      "Epoch: 18 / 20, MiniBatch: 29 / 196, Cost: 1.71772038936615\n",
      "Epoch: 18 / 20, MiniBatch: 59 / 196, Cost: 1.6810579299926758\n",
      "Epoch: 18 / 20, MiniBatch: 89 / 196, Cost: 1.6378397941589355\n",
      "Epoch: 18 / 20, MiniBatch: 119 / 196, Cost: 1.6435271501541138\n",
      "Epoch: 18 / 20, MiniBatch: 149 / 196, Cost: 1.635546326637268\n",
      "Epoch: 18 / 20, MiniBatch: 179 / 196, Cost: 1.638332486152649\n",
      "Accuracy of the network on the 10000 test images: 39.04\n",
      "Epoch: 19 / 20, MiniBatch: 29 / 196, Cost: 1.6354948282241821\n",
      "Epoch: 19 / 20, MiniBatch: 59 / 196, Cost: 1.5866830348968506\n",
      "Epoch: 19 / 20, MiniBatch: 89 / 196, Cost: 1.6559324264526367\n",
      "Epoch: 19 / 20, MiniBatch: 119 / 196, Cost: 1.6673970222473145\n",
      "Epoch: 19 / 20, MiniBatch: 149 / 196, Cost: 1.705017328262329\n",
      "Epoch: 19 / 20, MiniBatch: 179 / 196, Cost: 1.6794052124023438\n",
      "Accuracy of the network on the 10000 test images: 38.95\n",
      "Epoch: 20 / 20, MiniBatch: 29 / 196, Cost: 1.6708924770355225\n",
      "Epoch: 20 / 20, MiniBatch: 59 / 196, Cost: 1.7032257318496704\n",
      "Epoch: 20 / 20, MiniBatch: 89 / 196, Cost: 1.633966326713562\n",
      "Epoch: 20 / 20, MiniBatch: 119 / 196, Cost: 1.6499199867248535\n",
      "Epoch: 20 / 20, MiniBatch: 149 / 196, Cost: 1.7093318700790405\n",
      "Epoch: 20 / 20, MiniBatch: 179 / 196, Cost: 1.7326291799545288\n",
      "Accuracy of the network on the 10000 test images: 39.07\n",
      "Finshed Learning\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        #get the inputs\n",
    "        X, Y = data\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred = resnet50(X)\n",
    "        \n",
    "        #cost\n",
    "        cost = F.cross_entropy(pred, Y).to(device)\n",
    "        \n",
    "        #reduce the cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        lr_sche.step()\n",
    "\n",
    "        #print statistics\n",
    "        running_loss += cost.item()\n",
    "        if i % 30 == 29:\n",
    "            print('Epoch: {} / {}, MiniBatch: {} / {}, Cost: {}'.format(epoch + 1, 20, i, len(train_loader), cost.item()))\n",
    "            value_tracker(loss_plt, torch.Tensor([running_loss / 30]), torch.Tensor([i + epoch * len(train_loader)]))\n",
    "        running_loss = 0\n",
    "\n",
    "    #Check accuracy\n",
    "    acc = acc_check(resnet50, test_loader, epoch, save = 1)\n",
    "    value_tracker(acc_plt, torch.Tensor([acc]), torch.Tensor([epoch]))\n",
    "\n",
    "\n",
    "print('Finshed Learning')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 39.12\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        X, Y = data\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        #prediction\n",
    "        pred = resnet50(X)\n",
    "\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "\n",
    "        total += Y.size(0)\n",
    "\n",
    "        correct += (predicted == Y).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: {}'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10.1: CNN with MNIST Classifier\n",
    "\n",
    "**Jonathan Choi 2021**\n",
    "\n",
    "**[Deep Learning By Torch] End to End study scripts of Deep Learning by implementing code practice with Pytorch.**\n",
    "\n",
    "If you have an any issue, please PR below.\n",
    "\n",
    "[[Deep Learning By Torch] - Github @JonyChoi](https://github.com/jonychoi/Deep-Learning-By-Torch)\n",
    "\n",
    "Here, we are going to build simple CNN model for MNIST Classifying. To more understand about CNN, please refer before script \"10.0 - About CNN\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root = 'MNIST_data/',\n",
    "                             download = True,\n",
    "                             train = True,\n",
    "                             transform = transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(root = 'MNIST_data/',\n",
    "                            download = True,\n",
    "                            train = False,\n",
    "                            transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "training_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset = mnist_train, batch_size = batch_size, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Moment!\n",
    "\n",
    "Let's remind the PyTorch's CNN APIs.\n",
    "\n",
    "**CONV2D**\n",
    "\n",
    "> CLASS: ```torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)```\n",
    "\n",
    "Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "In the simplest case, the output value of the layer with input size $(N, C_{in}, H, W)$ and output $(N, C_{out}, H_{out}, W_{out})$  can be precisely described as:\n",
    "\n",
    "$$\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)$$\n",
    "\n",
    "\n",
    "where ⋆ is the valid 2D [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) operator, $N$ is a batch size, $C$ denotes a number of channels, $H$ is a height of input planes in pixels, and $W$ is width in pixels.\n",
    "\n",
    "This module supports [TensorFloat32](https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere).\n",
    "\n",
    "- ```stride``` controls the stride for the cross-correlation, a single number or a tuple.\n",
    "\n",
    "- ```padding``` controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or a tuple of ints giving the amount of implicit padding applied on both sides.\n",
    "\n",
    "- ```dilation``` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what dilation does.\n",
    "\n",
    "- ```groups``` controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,\n",
    "\n",
    "> - At groups=1, all inputs are convolved to all outputs.\n",
    "\n",
    "> - At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\n",
    "\n",
    "> - At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channels / in_channels)\n",
    "\n",
    "The parameters ```kernel_size```, ```stride```, ```padding```, ```dilation``` can either be:\n",
    "\n",
    "- a single ```int``` – in which case the same value is used for the height and width dimension\n",
    "\n",
    "- a ```tuple``` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
    "\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- **in_channels** (int) – Number of channels in the input image\n",
    "\n",
    "- **out_channels** (int) – Number of channels produced by the convolution\n",
    "\n",
    "- **kernel_size** (int or tuple) – Size of the convolving kernel\n",
    "\n",
    "- **stride** (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "\n",
    "- **padding** (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n",
    "\n",
    "- **padding_mode** (string, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n",
    "\n",
    "- **dilation** (int or tuple, optional) – Spacing between kernel elements. Default: 1\n",
    "\n",
    "- **groups** (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\n",
    "\n",
    "- **bias** (bool, optional) – If True, adds a learnable bias to the output. Default: True\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**BATCHNORM2D**\n",
    "\n",
    "> CLASS: ```torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)```\n",
    "\n",
    "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "The mean and standard-deviation are calculated per-dimension over the mini-batches and **γ** and **β** are learnable parameter vectors of size **C** (where C is the input size). By default, the elements of **γ** are set to 1 and the elements of **β** are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to **torch.var(input, unbiased=False)**.\n",
    "\n",
    "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.\n",
    "\n",
    "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "> This ```momentum``` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is $\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t$, $\\hat{x}$ is the estimated statistic and $x_t$ is the new observed value.\n",
    "​\n",
    "Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- **num_features** – $C$ from an expected input of size $(N, C, H, W)$\n",
    "\n",
    "- **eps** – a value added to the denominator for numerical stability. Default: 1e-5\n",
    "\n",
    "- **momentum** – the value used for the running_mean and running_var computation. Can be set to ```None``` for cumulative moving average (i.e. simple average). Default: 0.1\n",
    "\n",
    "- **affine** – a boolean value that when set to ```True```, this module has learnable affine parameters. Default: ```True```\n",
    "\n",
    "- **track_running_stats** – a boolean value that when set to True, this module tracks the running mean and variance, and when set to ```False```, this module does not track such statistics, and initializes statistics buffers ```running_mean``` and ```running_var``` as ```None```. When these buffers are ```None```, this module always uses batch statistics. in both training and eval modes. Default: ```True```\n",
    "\n",
    "---\n",
    "\n",
    "**MAXPOOL2D**\n",
    "\n",
    "> CLASS: ```torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)```\n",
    "\n",
    "Applies a 2D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "In the simplest case, the output value of the layer with input size $(N, C, H, W)$, output $(N, C, H_{out}, W_{out})$ and ```kernel_size``` $(kH, kW)$ can be precisely described as:\n",
    "\n",
    "$$\\begin{aligned} out(N_i, C_j, h, w) ={} & \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\ & \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m, \\text{stride[1]} \\times w + n) \\end{aligned}$$\n",
    "​\n",
    " \n",
    "If ```padding``` is non-zero, then the input is implicitly zero-padded on both sides for ```padding``` number of points. ```dilation``` controls the spacing between the kernel points. It is harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what ```dilation``` does.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "When ```ceil_mode=True```, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.\n",
    "\n",
    "The parameters ```kernel_size```, ```stride```, ```padding```, ```dilation``` can either be:\n",
    "\n",
    "a single ```int``` – in which case the same value is used for the height and width dimension\n",
    "\n",
    "a ```tuple``` of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- **kernel_size** – the size of the window to take a max over\n",
    "\n",
    "- **stride** – the stride of the window. Default value is kernel_size\n",
    "\n",
    "- **padding** – implicit zero padding to be added on both sides\n",
    "\n",
    "- **dilation** – a parameter that controls the stride of elements in the window\n",
    "\n",
    "- **return_indices** – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\n",
    "\n",
    "- **ceil_mode** – when True, will use ceil instead of floor to compute the output shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Layer 1: ImageInput Shape=(?, 1, 28, 28)\n",
    "        # Conv   : ImageOutput Shape=(?, 32, 28, 28)\n",
    "        # Pool   : ImageOutput Shape=(?, 32, 14, 14)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        # Layer 2: ImageInput Shape=(?, 32, 14, 14)\n",
    "        # Conv   : ImageOutput Shape=(?, 64, 14, 14)\n",
    "        # Pool   : ImageOutput Shape=(?, 64, 7, 7)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "        # Final Fully Connected\n",
    "        # 7 x 7 x 64 inputs => 10 outputs\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 10)\n",
    "        init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.size()) #torch.size([100, 64, 7, 7])\n",
    "        out = out.view(out.size(0), -1) # Flatten for FC\n",
    "        out = self.fc(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buddhalight\\envs\\buddhalight\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 20, cost: 0.218764\n",
      "Epoch: 2 / 20, cost: 0.059633\n",
      "Epoch: 3 / 20, cost: 0.044625\n",
      "Epoch: 4 / 20, cost: 0.034336\n",
      "Epoch: 5 / 20, cost: 0.029625\n",
      "Epoch: 6 / 20, cost: 0.024517\n",
      "Epoch: 7 / 20, cost: 0.020366\n",
      "Epoch: 8 / 20, cost: 0.017550\n",
      "Epoch: 9 / 20, cost: 0.014864\n",
      "Epoch: 10 / 20, cost: 0.012578\n",
      "Epoch: 11 / 20, cost: 0.009982\n",
      "Epoch: 12 / 20, cost: 0.009161\n",
      "Epoch: 13 / 20, cost: 0.007706\n",
      "Epoch: 14 / 20, cost: 0.008041\n",
      "Epoch: 15 / 20, cost: 0.005679\n",
      "Epoch: 16 / 20, cost: 0.004700\n",
      "Epoch: 17 / 20, cost: 0.003399\n",
      "Epoch: 18 / 20, cost: 0.005507\n",
      "Epoch: 19 / 20, cost: 0.003623\n",
      "Epoch: 20 / 20, cost: 0.002687\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "\n",
    "print('Learning started')\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # image is already size of (28 x 28), no reshape\n",
    "        # Label is not one-hot encoded\n",
    "\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        #prediction\n",
    "        pred = model(X)\n",
    "\n",
    "        #cost\n",
    "        cost = F.cross_entropy(pred, Y).to(device)\n",
    "\n",
    "        #Reduce the cost\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost\n",
    "\n",
    "    avg_cost = avg_cost / total_batch\n",
    "\n",
    "    print('Epoch: {} / {}, cost: {:.6f}'.format(epoch + 1, training_epochs, avg_cost))\n",
    "print('Learning Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Moment!\n",
    "\n",
    "Why we should ```.view(len(mnist_test), 1, 28 ,28)``` below despite of we didn't do that upon?\n",
    "\n",
    "=> since ```data_loader``` makes mini batches as size of size([100, 1, 28, 28]), we don't have to do that manually, but we don't use ```data_loader``` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9792999625205994\n"
     ]
    }
   ],
   "source": [
    "#Test model and check accuracy\n",
    "with torch.no_grad():\n",
    "    #print(mnist_test.data.shape) => torch.Size([10000, 28, 28])\n",
    "    X_test = mnist_test.data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.targets.to(device)\n",
    "\n",
    "    #prediction\n",
    "    pred = model(X_test)\n",
    "    correct_prediction = (torch.argmax(pred, 1) == Y_test)\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "\n",
    "    print('Accuracy: ', accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

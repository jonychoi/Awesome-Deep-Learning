{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8-4: Backpropagation with MNIST\n",
    "\n",
    "**Jonathan Choi 2021**\n",
    "\n",
    "**[Deep Learning By Torch] End to End study scripts of Deep Learning by implementing code practice with Pytorch.**\n",
    "\n",
    "If you have an any issue, please PR below.\n",
    "\n",
    "[[Deep Learning By Torch] - Github @JonyChoi](https://github.com/jonychoi/Deep-Learning-By-Torch)\n",
    "Here, we are going to learn how to backpropagation works at the deep inside. Before we used ```cost.backward()```, but here we are going to implement backward at the low level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root='MNIST_data/',\n",
    "                             download=True,\n",
    "                             train=True,\n",
    "                             transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(root='MNIST_data/',\n",
    "                            train = False,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.nn.Parameter(torch.Tensor(784, 30)).to(device)\n",
    "b1 = torch.nn.Parameter(torch.Tensor(30)).to(device)\n",
    "w2 = torch.nn.Parameter(torch.Tensor(30, 10)).to(device)\n",
    "b2 = torch.nn.Parameter(torch.Tensor(10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1696, -1.2966,  0.3153,  0.9196, -0.1853, -1.0896, -0.2633,  0.3830,\n",
       "        -0.6385,  1.4271], device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.normal_(w1)\n",
    "torch.nn.init.normal_(b1)\n",
    "torch.nn.init.normal_(w2)\n",
    "torch.nn.init.normal_(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Activation Functions and Its derivative terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    #derivative of the sigmoid function\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n",
      "858\n",
      "882\n",
      "868\n",
      "891\n",
      "889\n",
      "905\n",
      "896\n",
      "897\n"
     ]
    }
   ],
   "source": [
    "X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)[:1000]\n",
    "Y_test = mnist_test.test_labels.to(device)[:1000]\n",
    "\n",
    "i = 0\n",
    "\n",
    "while not i == 10000:\n",
    "    for X, Y in data_loader:\n",
    "        i += 1\n",
    "\n",
    "        #forward\n",
    "        X = X.view(-1 ,28* 28).to(device)\n",
    "        Y = torch.zeros((batch_size , 10)).scatter_(1, Y.unsqueeze(1), 1).to(device)\n",
    "\n",
    "        #one-hot\n",
    "        layer1 = torch.add(torch.matmul(X, w1), b1)\n",
    "        activation1 = sigmoid(layer1)\n",
    "\n",
    "        layer2 = torch.add(torch.matmul(activation1, w2), b2)\n",
    "        y_pred = sigmoid(layer2)\n",
    "\n",
    "        diff = y_pred - Y\n",
    "\n",
    "        #backward (back prop: Chain Rule)\n",
    "        diff_layer2 = diff * sigmoid_prime(layer2)\n",
    "        diff_b2 = diff_layer2\n",
    "        diff_w2 = torch.matmul(torch.transpose(activation1, 0, 1), diff_layer2)\n",
    "\n",
    "        diff_activation1 = torch.matmul(diff_layer2, torch.transpose(w2, 0, 1))\n",
    "        diff_layer1 = diff_activation1 * sigmoid_prime(layer1)\n",
    "        diff_b1 = diff_layer1\n",
    "        diff_w1 = torch.matmul(torch.transpose(X, 0, 1), diff_layer1)\n",
    "\n",
    "        w1 = w1 - learning_rate * diff_w1\n",
    "        b1 = b1 - learning_rate * torch.mean(diff_b1, 0)\n",
    "        w2 = w2 - learning_rate * diff_w2\n",
    "        b2 = b2 - learning_rate * torch.mean(diff_b2, 0)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            layer1 = torch.add(torch.matmul(X_test, w1), b1)\n",
    "            activation1 = sigmoid(layer1)\n",
    "            layer2 = torch.add(torch.matmul(activation1, w2), b2)\n",
    "            y_pred = sigmoid(layer2)\n",
    "            accuracy_mat = torch.argmax(y_pred, 1) == Y_test\n",
    "            accuracy_res = accuracy_mat.sum()\n",
    "            print(accuracy_res.item())\n",
    "\n",
    "        if i == 10000:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6.1: Softmax Classification\n",
    "\n",
    "**Jonathan Choi 2021**\n",
    "\n",
    "**[Deep Learning By Torch] End to End study scripts of Deep Learning by implementing code practice with Pytorch.**\n",
    "\n",
    "If you have an any issue, please PR below.\n",
    "\n",
    "[[Deep Learning By Torch] - Github @JonyChoi](https://github.com/jonychoi/Deep-Learning-By-Torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dfb087b450>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Convert numbers to probabilities with softmax.\n",
    "\n",
    "$ P(class=i) = \\frac{e^i}{\\sum e^i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has a ```softmax``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim = 0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are probabilities, they should add up to 1. Let's do a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss (Low-level)\n",
    "\n",
    "For multi-class classification, we use the cross entropy loss.\n",
    "\n",
    "$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $\n",
    "\n",
    "where $\\hat{y}$ is the predicted probability and $y$ is the correct probability (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create An Arbitrary Softmax values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad = True)\n",
    "hypothesis = F.softmax(z, dim = 1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Moment!\n",
    "\n",
    "** ```TORCH.RANDINT```\n",
    "\n",
    "torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).\n",
    "\n",
    "The shape of the tensor is defined by the variable argument size.\n",
    "\n",
    "At below, since we set the classes as 5 upon (```z=torch.rand(3, 5, requires_grad=True)```), the first argument was set to 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Arbitrary Correct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim = 1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss with ```torch.nn.functional```\n",
    "\n",
    "PyTorch has ```F.log_softmax()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Low Level Loss with Low level logsoftmax\n",
    "\n",
    "torch.log(F.softmax(z, dim=1))\n",
    "print((y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean())\n",
    "\n",
    "# Low Level Loss with High Level logsoftmax\n",
    "\n",
    "F.log_softmax(z, dim=1)\n",
    "print((y_one_hot * -F.log_softmax(z, dim=1)).sum(dim =1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has ```F.nll_loss()``` function that computes the negative likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level Loss with F.nll_loss(F.log_softmax)\n",
    "F.nll_loss(F.log_softmax(z, dim =1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has ```F.cross_entropy``` that combines ```F.log_softmax()``` and ```F.nll_loss()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level Loss with F.cross_entropy\n",
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Low-Level Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useless data for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 7],\n",
    "           [1, 7, 7, 7]]\n",
    "\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Moment!\n",
    "\n",
    "**TORCH.NN.FUNCTIONAL.ONE_HOTI**\n",
    "\n",
    "torch.nn.functional.one_hot(tensor, num_classes=-1) → LongTensor\n",
    "\n",
    "Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10000 Accruacy: 33.333336 Cost:1.098612\n",
      "Epoch: 1000/10000 Accruacy: 75.000000 Cost:0.585815\n",
      "Epoch: 2000/10000 Accruacy: 91.666672 Cost:0.405933\n",
      "Epoch: 3000/10000 Accruacy: 100.000000 Cost:0.245907\n",
      "Epoch: 4000/10000 Accruacy: 100.000000 Cost:0.207278\n",
      "Epoch: 5000/10000 Accruacy: 100.000000 Cost:0.178310\n",
      "Epoch: 6000/10000 Accruacy: 100.000000 Cost:0.155908\n",
      "Epoch: 7000/10000 Accruacy: 100.000000 Cost:0.138157\n",
      "Epoch: 8000/10000 Accruacy: 100.000000 Cost:0.123801\n",
      "Epoch: 9000/10000 Accruacy: 100.000000 Cost:0.111988\n",
      "Epoch: 10000/10000 Accruacy: 100.000000 Cost:0.102121\n"
     ]
    }
   ],
   "source": [
    "# Model Initialize\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "#set optimizer\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 10000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    #Hypothesis\n",
    "    pred = F.softmax(x_train.matmul(W) + b, dim = 1)\n",
    "\n",
    "    #one-hot-encoding\n",
    "    y_one_hot = torch.zeros_like(pred)\n",
    "    y_one_hot = y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    #y_one_hot == F.one_hot(y_train)\n",
    "\n",
    "    #Cost\n",
    "    cost = (y_one_hot * -torch.log(pred)).sum(dim =1).mean()\n",
    "\n",
    "    #Reduce Cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        x_one_hot = F.one_hot(torch.argmax(pred, dim = 1))\n",
    "        accuracy = (x_one_hot == y_one_hot).float().mean()*100\n",
    "        print('Epoch: {}/{} Accruacy: {:6f} Cost:{:.6f}'.format(epoch, nb_epochs, accuracy, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with ```nn.Module```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0/10000 Accuracy: 58.33333206176758 Cost: 1.920638\n",
      "Epoch 1000/10000 Accuracy: 91.66667175292969 Cost: 0.369083\n",
      "Epoch 2000/10000 Accuracy: 100.0 Cost: 0.173312\n",
      "Epoch 3000/10000 Accuracy: 100.0 Cost: 0.127871\n",
      "Epoch 4000/10000 Accuracy: 100.0 Cost: 0.100437\n",
      "Epoch 5000/10000 Accuracy: 100.0 Cost: 0.082302\n",
      "Epoch 6000/10000 Accuracy: 100.0 Cost: 0.069517\n",
      "Epoch 7000/10000 Accuracy: 100.0 Cost: 0.060061\n",
      "Epoch 8000/10000 Accuracy: 100.0 Cost: 0.052805\n",
      "Epoch 9000/10000 Accuracy: 100.0 Cost: 0.047074\n",
      "Epoch 10000/10000 Accuracy: 100.0 Cost: 0.042438\n"
     ]
    }
   ],
   "source": [
    "#Set Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs =10000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    #prediction\n",
    "    pred = model(x_train)\n",
    "\n",
    "    #cost\n",
    "    cost = F.cross_entropy(pred, y_train)\n",
    "\n",
    "    #reduce cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        accuracy = (F.one_hot(torch.argmax(F.softmax(pred, dim=1), dim=1)) == F.one_hot(y_train)).float().mean()*100\n",
    "        print('Epoch {:2d}/{} Accuracy: {} Cost: {:.6f}'.format(epoch, nb_epochs, accuracy, cost))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
